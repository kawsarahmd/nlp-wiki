In this lesson, we begin learning concrete algorithms for learning **word embeddings**.
Historically, deep learning researchers first used **complex algorithms** to learn embeddings. Over time, they discovered that **simpler and simpler** methods could achieve equally strong results—especially when trained on large datasets.

Some modern algorithms are so simple that if we presented them first, they might appear almost “magical.”
Therefore, we begin with the *slightly more complex* algorithms to build intuition, and then gradually simplify them.



## **1. Learning embeddings through a neural language model**

Suppose you're building a **language model** using a neural network. During training, you input a sequence such as:

> **I want a glass of orange** → predict the next word (**juice**).

Each word is represented by an **index in the vocabulary**, such as

* *I* → 4343
* *want* → 9665
* *a* → 1
  …and so on (these numbers are hypothetical indices).

You first convert each word into a **one-hot vector**.
For example, for the word *I* (index 4343), you create a vector of size 10,000 with a single 1 at position 4343.

You then multiply the one-hot vector by a **parameter matrix (E)**:

[
E \times O_{\text{word}} = \text{embedding vector}
]

This gives you a 300-dimensional word embedding.

You repeat this for all words in the input.

---

## **2. Using embeddings as input to a neural network**

You feed the embeddings into a neural network with:

* a hidden layer (parameters (W_1, B_1))
* a softmax output (parameters (W_2, B_2))
* the softmax predicts among all 10,000 words in the vocabulary

If the training example’s next word is “juice,” then the softmax target is the index representing *juice*.

If you use a window of the last **six** words, then the input size is:

[
6 \times 300 = 1800 \text{ dimensions}
]

More commonly, you fix a window size such as **four words**.
This ensures the model can work even with very long sentences because the input size is always fixed.

The parameters of the entire model are:

* the embedding matrix **E**
* the hidden layer weights **W₁, B₁**
* the softmax layer **W₂, B₂**

You use **backpropagation** to maximize the likelihood of correctly predicting the next word.

This approach learns good word embeddings because similar words appear in similar contexts.
For example:

> **orange juice**
> **apple juice**

To fit the training data well, the model naturally learns similar vectors for *orange* and *apple*, because both co-occur with *juice*.
Even rare fruits—like *durian*—may get similar vectors because they appear in similar contexts.

This was one of the earliest successful algorithms used for learning embedding matrices.



## **3. Generalizing the context**

Now, let’s consider a more complex sentence:

> **I want a glass of orange juice to go along with my cereal**

Previously, the model used the **last four words** as context to predict the target word (*juice*).
But for *learning embeddings*, researchers discovered many other useful context choices.

### **a. Using both left and right context**

Context = the four words on the left **and** the four words on the right.

Left:

> a glass of orange
> Right:
> to go along with

You feed embeddings of these words into a network to predict the middle word (*juice*).

### **b. Using just the immediately previous one word**

Context = just **one word**, e.g.:

> Context: **orange**
> Target: **juice**

This reduces complexity significantly.

### **c. Using one nearby word**

This leads to the **skip-gram model**.

Instead of predicting the next word, you take a word that appears anywhere near your target:

Example:

> Context word: **glass**
> Predict a word near it: **orange**, **of**, **a**, etc.

This approach uses very simple contexts (just one word) yet learns excellent embeddings.



## **4. Why many contexts work well**

If your goal is to build a full **language model**, using the last few words as context is natural.
But if your primary goal is **learning embeddings**, then:

* left + right words
* single previous word
* single nearby word

…all provide useful learning signals.

These methods all encourage similar meanings to have similar embeddings based on the contexts they appear in.



## **5. Summary**

* You can learn embeddings by training a language model:
  **context → predict target word**

* Early methods used several preceding words; later methods used simpler contexts.

* Even very simple algorithms—such as using only one nearby word (skip-gram)—can produce high-quality embeddings.

* All these methods help the algorithm learn that words appearing in similar contexts should have **similar embeddings**.

