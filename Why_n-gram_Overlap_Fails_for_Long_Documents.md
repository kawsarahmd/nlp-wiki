
## Why n-gram Overlap Fails for Long Documents

Retrieving documents using n-gram overlap means ranking documents based on how many small word sequences they share with the user’s query. An n-gram is simply a group of consecutive words. For example, consider the query “machine learning model training.” If we use bigrams, the query is split into “machine learning,” “learning model,” and “model training.” A retrieval system then checks how many of these exact word pairs appear in each document and uses that count as a relevance score. The idea is simple: more shared n-grams should mean higher relevance.

This approach works well when documents and queries are roughly the same length. Suppose a document is a short paragraph describing how to train a machine learning model. It might contain two of the three query bigrams. Another unrelated short document is unlikely to contain those same sequences. In this case, n-gram overlap provides a strong and reliable signal because random matches are rare and meaningful overlap stands out clearly.

The problem appears when documents are much longer than the query. Imagine the same query with three bigrams, but now consider a long technical report containing thousands of words about data science, statistics, optimization, deployment, and infrastructure. Because this document is very long, it naturally contains many different word combinations. Even if it only briefly mentions model training, it is still likely to include all three query bigrams somewhere in the text. The system now assigns it a high overlap score, even though only a small fraction of the document is actually relevant.

To make this concrete, imagine two documents. Document A is a short tutorial of 150 words focused entirely on training machine learning models. It contains two of the three query bigrams. Document B is a 5,000-word handbook on artificial intelligence that touches many topics and includes all three bigrams once. A naive n-gram overlap system would rank Document B higher because three is greater than two. From a user’s perspective, this result is clearly wrong. The short tutorial is far more useful, but the scoring system is biased toward the longer document simply because it had more chances to match.

This happens because longer documents generate far more n-grams than shorter ones. As document length increases, the probability of matching any given query n-gram increases, even without strong topical relevance. Eventually, many long documents end up with similar overlap scores, making it difficult to tell which ones are actually relevant. At that point, overlap stops being a meaningful ranking signal and becomes a proxy for document length.

This exact issue is why modern retrieval systems do not rely on raw overlap counts. Methods like TF-IDF and BM25 adjust for document length and reduce the impact of single accidental matches. Instead of asking “how many times does this n-gram appear,” they ask “is this n-gram unusually important in this document compared to others.” This normalization step restores the discriminative power that naive overlap loses.

The same problem becomes even more visible in Retrieval-Augmented Generation systems. In RAG, the query is often a short question, while documents may be entire PDFs or long knowledge base articles. If retrieval is based on n-gram overlap alone, the system tends to retrieve long chunks that happen to contain the query phrases, even if the relevant information is buried deep inside. The language model then receives noisy context, which leads to vague or incorrect answers. In practice, this is one of the most common reasons RAG systems perform poorly.

In summary, n-gram overlap is easy to understand and useful in limited scenarios, but it breaks down when document lengths vary significantly. The core issue is simple: longer documents have more opportunities to match, so overlap scores become inflated and unreliable. This is why overlap must be normalized, restricted to passages, or combined with other retrieval signals. Without those corrections, n-gram overlap cannot reliably separate relevant documents from irrelevant ones at scale.

#### Why Beating BM25 Is So Hard
Aravind Srinivas, the CEO of Perplexity, comment that “making a genuine improvement over BM25 or full-text search is hard” is not a casual remark. It reflects a hard, often uncomfortable reality that many modern AI search and RAG teams rediscover after months of experimentation.

BM25 looks simple, but it encodes decades of retrieval research. It solves several core problems at once: term importance, document length bias, and noise from frequent words. Most new retrieval ideas underestimate how much value comes from these basics being done correctly. Dense embeddings often look impressive in demos, but when you test them rigorously across large, messy corpora, they frequently lose to BM25 on precision, stability, and debuggability. That gap surprises many teams.

One reason improving over BM25 is hard is that relevance in real systems is often lexical, not semantic. Users care about exact terms, entities, product names, error messages, legal phrases, or code symbols. BM25 is extremely strong at matching these signals. Dense models may understand semantic similarity, but they are weaker at exact matching unless heavily engineered. In practice, users punish systems that miss obvious keyword matches more than systems that miss subtle semantic ones.

Another reason is robustness. BM25 degrades gracefully. If data distribution shifts, vocabulary changes, or documents are noisy, BM25 still works reasonably well. Dense retrievers are brittle. Small changes in data, chunking strategy, or embedding model can cause large relevance swings. This makes dense-only systems risky in production, especially at scale.

There is also an evaluation problem. Many claimed “improvements” over BM25 come from cherry-picked benchmarks or short queries against clean datasets. In real search traffic, queries are ambiguous, incomplete, and sometimes wrong. BM25 handles this mess better than expected because it relies on surface signals users actually type. Dense retrieval often looks better on academic metrics but worse in user satisfaction.

This is why Perplexity, Google, OpenAI, and others use hybrid systems. They do not replace BM25. They layer on top of it. Sparse retrieval ensures recall and lexical grounding. Dense retrieval helps with semantic expansion. Rerankers clean up the final ordering. The innovation is not beating BM25 outright, but combining signals so that BM25’s strengths are preserved while its weaknesses are compensated.

The deeper takeaway from Aravind’s statement is strategic. If you claim you have “beaten BM25,” the bar is extremely high. You must show improvements across diverse queries, long-tail cases, adversarial inputs, latency constraints, and cost limits. Most systems that claim victory fail one or more of these dimensions. That is why BM25 remains a baseline everywhere and why serious search teams treat it as infrastructure, not as a competitor.

In short, BM25 is not impressive because it is fancy. It is impressive because it is hard to replace. Any retrieval system that ignores this lesson usually ends up rediscovering it the expensive way.
