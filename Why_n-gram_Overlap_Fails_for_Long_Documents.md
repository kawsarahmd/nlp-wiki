
## Why n-gram Overlap Fails for Long Documents

Retrieving documents using n-gram overlap means ranking documents based on how many small word sequences they share with the user’s query. An n-gram is simply a group of consecutive words. For example, consider the query “machine learning model training.” If we use bigrams, the query is split into “machine learning,” “learning model,” and “model training.” A retrieval system then checks how many of these exact word pairs appear in each document and uses that count as a relevance score. The idea is simple: more shared n-grams should mean higher relevance.

This approach works well when documents and queries are roughly the same length. Suppose a document is a short paragraph describing how to train a machine learning model. It might contain two of the three query bigrams. Another unrelated short document is unlikely to contain those same sequences. In this case, n-gram overlap provides a strong and reliable signal because random matches are rare and meaningful overlap stands out clearly.

The problem appears when documents are much longer than the query. Imagine the same query with three bigrams, but now consider a long technical report containing thousands of words about data science, statistics, optimization, deployment, and infrastructure. Because this document is very long, it naturally contains many different word combinations. Even if it only briefly mentions model training, it is still likely to include all three query bigrams somewhere in the text. The system now assigns it a high overlap score, even though only a small fraction of the document is actually relevant.

To make this concrete, imagine two documents. Document A is a short tutorial of 150 words focused entirely on training machine learning models. It contains two of the three query bigrams. Document B is a 5,000-word handbook on artificial intelligence that touches many topics and includes all three bigrams once. A naive n-gram overlap system would rank Document B higher because three is greater than two. From a user’s perspective, this result is clearly wrong. The short tutorial is far more useful, but the scoring system is biased toward the longer document simply because it had more chances to match.

This happens because longer documents generate far more n-grams than shorter ones. As document length increases, the probability of matching any given query n-gram increases, even without strong topical relevance. Eventually, many long documents end up with similar overlap scores, making it difficult to tell which ones are actually relevant. At that point, overlap stops being a meaningful ranking signal and becomes a proxy for document length.

This exact issue is why modern retrieval systems do not rely on raw overlap counts. Methods like TF-IDF and BM25 adjust for document length and reduce the impact of single accidental matches. Instead of asking “how many times does this n-gram appear,” they ask “is this n-gram unusually important in this document compared to others.” This normalization step restores the discriminative power that naive overlap loses.

The same problem becomes even more visible in Retrieval-Augmented Generation systems. In RAG, the query is often a short question, while documents may be entire PDFs or long knowledge base articles. If retrieval is based on n-gram overlap alone, the system tends to retrieve long chunks that happen to contain the query phrases, even if the relevant information is buried deep inside. The language model then receives noisy context, which leads to vague or incorrect answers. In practice, this is one of the most common reasons RAG systems perform poorly.

In summary, n-gram overlap is easy to understand and useful in limited scenarios, but it breaks down when document lengths vary significantly. The core issue is simple: longer documents have more opportunities to match, so overlap scores become inflated and unreliable. This is why overlap must be normalized, restricted to passages, or combined with other retrieval signals. Without those corrections, n-gram overlap cannot reliably separate relevant documents from irrelevant ones at scale.
